{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping executive actions by president Donald Trump starting Jan 20, 2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 of 5...\n",
      "Scraping page 2 of 5...\n",
      "Scraping page 3 of 5...\n",
      "Scraping page 4 of 5...\n",
      "Scraping page 5 of 5...\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/ending-illegal-discrimination-and-restoring-merit-based-opportunity/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/keeping-americans-safe-in-aviation/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/guaranteeing-the-states-protection-against-invasion/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/restoring-names-that-honor-american-greatness/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/designating-cartels-and-other-organizations-as-foreign-terrorist-organizations-and-specially-designated-global-terrorists/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/reforming-the-federal-hiring-process-and-restoring-merit-to-government-service/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/ending-radical-and-wasteful-government-dei-programs-and-preferencing/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/defending-women-from-gender-ideology-extremism-and-restoring-biological-truth-to-the-federal-government/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/establishing-and-implementing-the-presidents-department-of-government-efficiency/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/america-first-policy-directive-to-the-secretary-of-state/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/protecting-the-united-states-from-foreign-terrorists-and-othernational-security-and-public-safety-threats/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/unleashing-alaskas-extraordinary-resource-potential/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/protecting-the-american-people-against-invasion/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/the-organization-for-economic-co-operation-and-development-oecd-global-tax-deal-global-tax-deal/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/organization-of-the-national-security-council-and-subcommittees/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/reevaluating-and-realigning-united-states-foreign-aid/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/temporary-withdrawal-of-all-areas-on-the-outer-continental-shelf-from-offshore-wind-leasing-and-review-of-the-federal-governments-leasing-and-permitting-practices-for-wind-projects/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/declaring-a-national-energy-emergency/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/restoring-accountability-for-career-senior-executives/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/promoting-beautiful-federal-civic-architecture/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/restoring-the-death-penalty-and-protecting-public-safety/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/putting-people-over-fish-stopping-radical-environmentalism-to-provide-water-to-southern-california/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/securing-our-borders/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/protecting-the-meaning-and-value-of-american-citizenship/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/realigning-the-united-states-refugee-admissions-program/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/unleashing-american-energy/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/clarifying-the-militarys-role-in-protecting-the-territorial-integrity-of-the-united-states/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/america-first-trade-policy/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/memorandum-to-resolve-the-backlog-of-security-clearances-for-executive-office-of-the-president-personnel/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/declaring-a-national-emergency-at-the-southern-border-of-the-united-states/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/holding-former-government-officials-accountablefor-election-interference-and-improper-disclosure-of-sensitive-governmental-information/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/restoring-accountability-to-policy-influencing-positions-within-the-federal-workforce/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/withdrawing-the-united-states-from-the-worldhealth-organization/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/application-of-protecting-americans-from-foreign-adversary-controlled-applications-act-to-tiktok/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/granting-pardons-and-commutation-of-sentences-for-certain-offenses-relating-to-the-events-at-or-near-the-united-states-capitol-on-january-6-2021/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/putting-america-first-in-international-environmental-agreements/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/delivering-emergency-price-relief-for-american-families-and-defeating-the-cost-of-living-crisis/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/hiring-freeze/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/regulatory-freeze-pending-review/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/return-to-in-person-work/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/ending-the-weaponization-of-the-federal-government/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/restoring-freedom-of-speech-and-ending-federal-censorship/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/initial-rescissions-of-harmful-executive-orders-and-actions/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/flying-the-flag-of-the-united-states-at-full-staff-on-inauguration-day/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/designation-of-chairmen-and-acting-chairmen/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/designation-of-acting-leaders/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/sub-cabinet-appointments/\n",
      "Scraping URL: https://www.whitehouse.gov/presidential-actions/2025/01/cabinet-and-cabinet-level-appointments/\n",
      "Scraping complete. New data saved.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "# Base URL of the webpage to scrape\n",
    "base_url = \"https://www.whitehouse.gov/presidential-actions/page/{}/\"\n",
    "\n",
    "# Initialize a list to store post data\n",
    "data = []\n",
    "\n",
    "# Function to create a session with retry strategy\n",
    "def create_session():\n",
    "    session = requests.Session()\n",
    "    retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
    "    session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "    return session\n",
    "\n",
    "# Function to scrape a single page\n",
    "def scrape_page(session, page_number, scraped_links):\n",
    "    url = base_url.format(page_number)\n",
    "    response = session.get(url)\n",
    "    response.raise_for_status()  # Ensure the request was successful\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    posts = soup.find_all('li', class_='wp-block-post')\n",
    "\n",
    "    for post in posts:\n",
    "        title_element = post.find('h2', class_='wp-block-post-title')\n",
    "        link_element = title_element.find('a')\n",
    "        date_element = post.find('time')\n",
    "        category_element = post.find('div', class_='taxonomy-category').find('a')\n",
    "\n",
    "        title = title_element.get_text(strip=True)\n",
    "        link = link_element['href']\n",
    "        date = date_element['datetime']\n",
    "        category = category_element.get_text(strip=True)\n",
    "\n",
    "        # Only scrape if the link hasn't been scraped before\n",
    "        if link not in scraped_links:\n",
    "            data.append({\n",
    "                'Title': title,\n",
    "                'Link': link,\n",
    "                'Date': date,\n",
    "                'Category': category\n",
    "            })\n",
    "            scraped_links.add(link)  # Add to the set of scraped links\n",
    "\n",
    "# Function to scrape details from a single URL\n",
    "def scrape_details(session, url):\n",
    "    response = session.get(url)\n",
    "    response.raise_for_status()  # Ensure the request was successful\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        name = soup.find('div', class_=\"wp-block-whitehouse-topper__eyebrow\").text.strip()\n",
    "    except AttributeError:\n",
    "        name = None\n",
    "\n",
    "    try:\n",
    "        headline = soup.find('h1', class_=\"wp-block-whitehouse-topper__headline\").text.strip().title()\n",
    "    except AttributeError:\n",
    "        headline = None\n",
    "\n",
    "    try:\n",
    "        date = soup.find('div', class_=\"wp-block-post-date\").text.strip()\n",
    "    except AttributeError:\n",
    "        date = None\n",
    "\n",
    "    try:\n",
    "        byline = soup.find('div', class_=\"wp-block-whitehouse-topper__meta--byline\").text.strip()\n",
    "    except AttributeError:\n",
    "        byline = None\n",
    "\n",
    "    # Start of the content extraction\n",
    "    try:\n",
    "        content_section = soup.find(\n",
    "            'div', class_=\"entry-content wp-block-post-content has-global-padding is-layout-constrained wp-block-post-content-is-layout-constrained\"\n",
    "        )\n",
    "        if content_section:\n",
    "            content_parts = []\n",
    "\n",
    "            # Extract text from all `p` elements\n",
    "            for p in content_section.find_all('p'):\n",
    "                content_parts.append(p.get_text(strip=True))\n",
    "\n",
    "            # Extract text from tables inside `figure` elements and capture all rows and columns\n",
    "            for table in content_section.find_all('table'):\n",
    "                for row in table.find_all('tr'):\n",
    "                    cells = [cell.get_text(strip=True) for cell in row.find_all(['td', 'th'])]\n",
    "                    content_parts.append(' | '.join(cells))  # Combine table row cells with a delimiter\n",
    "\n",
    "            # Include other block-level elements like `div`, `span` (e.g., for additional text content)\n",
    "            for div in content_section.find_all(['div', 'span']):\n",
    "                div_text = div.get_text(strip=True)\n",
    "                if div_text:  # Only append non-empty text\n",
    "                    content_parts.append(div_text)\n",
    "\n",
    "            # Combine all parts into a single content string, separated by newlines\n",
    "            content = '\\n'.join(content_parts)\n",
    "\n",
    "        else:\n",
    "            content = None\n",
    "\n",
    "    except AttributeError:\n",
    "        content = None\n",
    "\n",
    "    return {\n",
    "        'Name': name,\n",
    "        'Headline': headline,\n",
    "        'Date': date,\n",
    "        'Byline': byline,\n",
    "        'Content': content\n",
    "    }\n",
    "\n",
    "# Load the already scraped links (if any) from a CSV or file\n",
    "def load_scraped_links():\n",
    "    try:\n",
    "        # Load previously scraped data from a CSV file (or database)\n",
    "        df = pd.read_csv('scraped_whitehouse_posts.csv')\n",
    "        return set(df['Link'])  # Return a set of links from the CSV\n",
    "    except FileNotFoundError:\n",
    "        return set()  # If no file exists, return an empty set\n",
    "\n",
    "# Save new data to a CSV file\n",
    "def save_scraped_data(new_data):\n",
    "    df = pd.DataFrame(new_data)\n",
    "    df.to_csv('scraped_whitehouse_posts.csv', mode='a', header=False, index=False)\n",
    "\n",
    "# Create a session\n",
    "session = create_session()\n",
    "\n",
    "# Load previously scraped links\n",
    "scraped_links = load_scraped_links()\n",
    "\n",
    "# Find the total number of pages\n",
    "initial_response = session.get(base_url.format(1))\n",
    "initial_response.raise_for_status()\n",
    "soup = BeautifulSoup(initial_response.content, 'html.parser')\n",
    "pagination = soup.find('div', class_='wp-block-query-pagination-numbers')\n",
    "\n",
    "# Extract the number of pages\n",
    "if pagination:\n",
    "    pages = pagination.find_all('a', class_='page-numbers')\n",
    "    total_pages = max(int(page.get_text()) for page in pages if page.get_text().isdigit())\n",
    "else:\n",
    "    total_pages = 1\n",
    "\n",
    "# Loop through all pages and scrape data\n",
    "for page_number in range(1, total_pages + 1):\n",
    "    print(f\"Scraping page {page_number} of {total_pages}...\")\n",
    "    scrape_page(session, page_number, scraped_links)\n",
    "    time.sleep(1)  # Be polite and avoid overwhelming the server\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize a list to store detailed scraped data\n",
    "scraped_data = []\n",
    "\n",
    "# Loop through each URL in the DataFrame and scrape details\n",
    "for index, row in df.iterrows():\n",
    "    url = row['Link']\n",
    "    try:\n",
    "        print(f\"Scraping URL: {url}\")\n",
    "        details = scrape_details(session, url)\n",
    "        details.update({\n",
    "            'Title': row['Title'],\n",
    "            'Date': row['Date'],\n",
    "            'Category': row['Category'],\n",
    "            'Link': url\n",
    "        })\n",
    "        scraped_data.append(details)\n",
    "        time.sleep(1)  # Be polite and avoid overwhelming the server\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping URL {url}: {e}\")\n",
    "\n",
    "# Save the final detailed data\n",
    "save_scraped_data(scraped_data)\n",
    "\n",
    "print(\"Scraping complete. New data saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
