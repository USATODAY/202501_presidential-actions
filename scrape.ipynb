{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping executive actions by president Donald Trump starting Jan 20, 2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 of 5...\n",
      "Scraping page 2 of 5...\n",
      "Scraping page 3 of 5...\n",
      "Scraping page 4 of 5...\n",
      "Scraping page 5 of 5...\n",
      "Scraping complete. New data saved.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "# Base URL of the webpage to scrape\n",
    "base_url = \"https://www.whitehouse.gov/presidential-actions/page/{}/\"\n",
    "\n",
    "# Initialize a list to store post data\n",
    "data = []\n",
    "\n",
    "# Function to create a session with retry strategy\n",
    "def create_session():\n",
    "    session = requests.Session()\n",
    "    retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
    "    session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "    return session\n",
    "\n",
    "# Function to scrape a single page\n",
    "def scrape_page(session, page_number, scraped_links):\n",
    "    url = base_url.format(page_number)\n",
    "    response = session.get(url)\n",
    "    response.raise_for_status()  # Ensure the request was successful\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    posts = soup.find_all('li', class_='wp-block-post')\n",
    "\n",
    "    for post in posts:\n",
    "        title_element = post.find('h2', class_='wp-block-post-title')\n",
    "        link_element = title_element.find('a')\n",
    "        date_element = post.find('time')\n",
    "        category_element = post.find('div', class_='taxonomy-category').find('a')\n",
    "\n",
    "        title = title_element.get_text(strip=True)\n",
    "        link = link_element['href']\n",
    "        date = date_element['datetime']\n",
    "        category = category_element.get_text(strip=True)\n",
    "\n",
    "        # Only scrape if the link hasn't been scraped before\n",
    "        if link not in scraped_links:\n",
    "            data.append({\n",
    "                'Title': title,\n",
    "                'Link': link,\n",
    "                'Date': date,\n",
    "                'Category': category\n",
    "            })\n",
    "            scraped_links.add(link)  # Add to the set of scraped links\n",
    "\n",
    "# Function to scrape details from a single URL\n",
    "def scrape_details(session, url):\n",
    "    response = session.get(url)\n",
    "    response.raise_for_status()  # Ensure the request was successful\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        name = soup.find('div', class_=\"wp-block-whitehouse-topper__eyebrow\").text.strip()\n",
    "    except AttributeError:\n",
    "        name = None\n",
    "\n",
    "    try:\n",
    "        headline = soup.find('h1', class_=\"wp-block-whitehouse-topper__headline\").text.strip().title()\n",
    "    except AttributeError:\n",
    "        headline = None\n",
    "\n",
    "    try:\n",
    "        date = soup.find('div', class_=\"wp-block-post-date\").text.strip()\n",
    "    except AttributeError:\n",
    "        date = None\n",
    "\n",
    "    try:\n",
    "        byline = soup.find('div', class_=\"wp-block-whitehouse-topper__meta--byline\").text.strip()\n",
    "    except AttributeError:\n",
    "        byline = None\n",
    "\n",
    "    try:\n",
    "        content = ' '.join([p.text.strip() for p in soup.find(\n",
    "            'div', class_=\"entry-content wp-block-post-content has-global-padding is-layout-constrained wp-block-post-content-is-layout-constrained\"\n",
    "        ).find_all('p')])\n",
    "    except AttributeError:\n",
    "        content = None\n",
    "\n",
    "    return {\n",
    "        'Name': name,\n",
    "        'Headline': headline,\n",
    "        'Date': date,\n",
    "        'Byline': byline,\n",
    "        'Content': content\n",
    "    }\n",
    "\n",
    "# Load the already scraped links (if any) from a CSV or file\n",
    "def load_scraped_links():\n",
    "    try:\n",
    "        # Load previously scraped data from a CSV file (or database)\n",
    "        df = pd.read_csv('scraped_whitehouse_posts.csv')\n",
    "        return set(df['Link'])  # Return a set of links from the CSV\n",
    "    except FileNotFoundError:\n",
    "        return set()  # If no file exists, return an empty set\n",
    "\n",
    "# Save new data to a CSV file\n",
    "def save_scraped_data(new_data):\n",
    "    df = pd.DataFrame(new_data)\n",
    "    df.to_csv('scraped_whitehouse_posts.csv', mode='a', header=False, index=False)\n",
    "\n",
    "# Create a session\n",
    "session = create_session()\n",
    "\n",
    "# Load previously scraped links\n",
    "scraped_links = load_scraped_links()\n",
    "\n",
    "# Find the total number of pages\n",
    "initial_response = session.get(base_url.format(1))\n",
    "initial_response.raise_for_status()\n",
    "soup = BeautifulSoup(initial_response.content, 'html.parser')\n",
    "pagination = soup.find('div', class_='wp-block-query-pagination-numbers')\n",
    "\n",
    "# Extract the number of pages\n",
    "if pagination:\n",
    "    pages = pagination.find_all('a', class_='page-numbers')\n",
    "    total_pages = max(int(page.get_text()) for page in pages if page.get_text().isdigit())\n",
    "else:\n",
    "    total_pages = 1\n",
    "\n",
    "# Loop through all pages and scrape data\n",
    "for page_number in range(1, total_pages + 1):\n",
    "    print(f\"Scraping page {page_number} of {total_pages}...\")\n",
    "    scrape_page(session, page_number, scraped_links)\n",
    "    time.sleep(1)  # Be polite and avoid overwhelming the server\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize a list to store detailed scraped data\n",
    "scraped_data = []\n",
    "\n",
    "# Loop through each URL in the DataFrame and scrape details\n",
    "for index, row in df.iterrows():\n",
    "    url = row['Link']\n",
    "    try:\n",
    "        print(f\"Scraping URL: {url}\")\n",
    "        details = scrape_details(session, url)\n",
    "        details.update({\n",
    "            'Title': row['Title'],\n",
    "            'Date': row['Date'],\n",
    "            'Category': row['Category'],\n",
    "            'Link': url\n",
    "        })\n",
    "        scraped_data.append(details)\n",
    "        time.sleep(1)  # Be polite and avoid overwhelming the server\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping URL {url}: {e}\")\n",
    "\n",
    "# Save the final detailed data\n",
    "save_scraped_data(scraped_data)\n",
    "\n",
    "print(\"Scraping complete. New data saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
